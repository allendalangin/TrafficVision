{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955ec3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In src/backbone.py\n",
    "from mindspore import nn\n",
    "from mindcv.models import create_model\n",
    "\n",
    "def create_efficientnet_backbone():\n",
    "    \"\"\"\n",
    "    Loads a pre-trained EfficientNet-B1 model and prepares it\n",
    "    to be used as a backbone for an object detection network.\n",
    "    \"\"\"\n",
    "    print(\"Loading pre-trained EfficientNet-B1 backbone...\")\n",
    "    \n",
    "    # 1. Load the pre-trained classification model from mindcv\n",
    "    backbone = create_model('efficientnet_b1', pretrained=True)\n",
    "    \n",
    "    # 2. Remove the final classification layer (the \"head\")\n",
    "    #    We replace it with an Identity layer that does nothing.\n",
    "    #    This exposes the powerful features from the main body of the network.\n",
    "    backbone.head = nn.Identity()\n",
    "    \n",
    "    print(\"Backbone created successfully!\")\n",
    "    return backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cfff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2872:2020,MainProcess):2025-10-12-20:42:58.347.000 [mindspore\\context.py:1412] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing COCO dataset...\n",
      "Initializing COCO dataset loader...\n",
      "Dataset initialized with 118287 images.\n",
      "Loading pre-trained EfficientNet-B1 backbone...\n",
      "Backbone created successfully!\n",
      "\n",
      "--- Testing Data Flow ---\n",
      "Loaded a preprocessed image with shape: (1, 3, 224, 224)\n",
      "Backbone produced a feature map with shape: (1, 1000)\n",
      "\n",
      "This feature map is what an object detection head would use to find bounding boxes.\n"
     ]
    }
   ],
   "source": [
    "# In main.py\n",
    "import mindspore as ms\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "from mindspore.dataset.transforms import TypeCast\n",
    "# --- IMPORT THESE NEW TRANSFORMS ---\n",
    "import mindspore.dataset.vision as vision\n",
    "\n",
    "# Import your custom dataset loader and new backbone creator\n",
    "from src.coco_dataset import CocoDataset\n",
    "from src.backbone import create_efficientnet_backbone\n",
    "\n",
    "# Set the context to CPU\n",
    "ms.set_context(device_target=\"CPU\")\n",
    "\n",
    "## 1. Prepare your COCO Dataset\n",
    "print(\"Initializing COCO dataset...\")\n",
    "annotations_path = './coco2017/annotations/instances_train2017.json'\n",
    "images_path = './coco2017/train2017'\n",
    "coco_ds = CocoDataset(annotations_file=annotations_path, images_dir=images_path)\n",
    "\n",
    "dataset = GeneratorDataset(\n",
    "    source=coco_ds,\n",
    "    column_names=[\"image\", \"boxes\", \"labels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# Define a list of transformations to apply to each image.\n",
    "# The order is important.\n",
    "transforms = [\n",
    "    vision.Resize((224, 224)),      # 1. Resize the image to 224x224\n",
    "    TypeCast(ms.float32),           # 2. Convert the data type to float\n",
    "    vision.HWC2CHW()                # 3. Change shape from (H, W, C) to (C, H, W)\n",
    "]\n",
    "\n",
    "# Apply the transformations to the 'image' column.\n",
    "dataset = dataset.map(operations=transforms, input_columns=\"image\")\n",
    "\n",
    "\n",
    "# Now that images are a fixed size, you can batch them.\n",
    "dataset = dataset.batch(batch_size=1) # Using batch size of 1 for this test\n",
    "\n",
    "## 2. Create your pre-trained backbone\n",
    "backbone_model = create_efficientnet_backbone()\n",
    "\n",
    "## 3. Test the flow of data\n",
    "print(\"\\n--- Testing Data Flow ---\")\n",
    "\n",
    "# Get one batch of data from your COCO loader\n",
    "data_iterator = dataset.create_dict_iterator()\n",
    "data_batch = next(data_iterator)\n",
    "\n",
    "image_tensor = data_batch['image']\n",
    "ground_truth_boxes = data_batch['boxes']\n",
    "\n",
    "print(f\"Loaded a preprocessed image with shape: {image_tensor.shape}\") # Should be (1, 3, 224, 224)\n",
    "\n",
    "# 4. Pass the image through the backbone to get features\n",
    "#    In a real model, these features would go to the detection head.\n",
    "feature_map = backbone_model(image_tensor)\n",
    "\n",
    "print(f\"Backbone produced a feature map with shape: {feature_map.shape}\")\n",
    "print(\"\\nThis feature map is what an object detection head would use to find bounding boxes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a015eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2872:2020,MainProcess):2025-10-12-20:58:18.725.000 [mindspore\\context.py:1412] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n",
      "[WARNING] ME(2872:2020,MainProcess):2025-10-12-20:58:18.725.000 [mindspore\\context.py:1412] For 'context.set_context', the parameter 'pynative_synchronize' will be deprecated and removed in a future version. Please use the api mindspore.runtime.launch_blocking() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing COCO dataset...\n",
      "Initializing COCO dataset loader...\n",
      "Dataset initialized with 118287 images.\n",
      "\n",
      "Creating the full TrafficVisionDetector model...\n",
      "Loading pre-trained EfficientNet-B1 backbone...\n",
      "Backbone created successfully!\n",
      "\n",
      "--- Testing Full Model ---\n",
      "Loaded a preprocessed image with shape: (1, 3, 224, 224)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For primitive[Conv2D], the x shape size must be equal to 4, but got 2.\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore\\core\\utils\\check_convert_utils.cc:626 mindspore::CheckAndConvertUtils::CheckInteger\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded a preprocessed image with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_tensor.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 4. Pass the image through the full model to get final predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m box_predictions, class_predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModel produced Box Predictions with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbox_predictions.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel produced Class Predictions with shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_predictions.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\venv\\Lib\\site-packages\\mindspore\\nn\\cell.py:1355\u001b[39m, in \u001b[36mCell.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.requires_grad \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dynamic_shape_inputs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mixed_precision_type):\n\u001b[32m   1353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._forward_pre_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_hook \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1354\u001b[39m             \u001b[38;5;28mself\u001b[39m._shard_fn \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recompute_cell \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.has_bprop \u001b[38;5;129;01mand\u001b[39;00m _pynative_executor.requires_grad())):\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_construct(*args, **kwargs)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._complex_call(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\src\\detector.py:25\u001b[39m, in \u001b[36mTrafficVisionDetector.construct\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m     22\u001b[39m features = \u001b[38;5;28mself\u001b[39m.backbone(image)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Pass the features through the head to get the final predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m box_preds, class_preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m box_preds, class_preds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\venv\\Lib\\site-packages\\mindspore\\nn\\cell.py:1355\u001b[39m, in \u001b[36mCell.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.requires_grad \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dynamic_shape_inputs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mixed_precision_type):\n\u001b[32m   1353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._forward_pre_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_hook \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1354\u001b[39m             \u001b[38;5;28mself\u001b[39m._shard_fn \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recompute_cell \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.has_bprop \u001b[38;5;129;01mand\u001b[39;00m _pynative_executor.requires_grad())):\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_construct(*args, **kwargs)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._complex_call(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\src\\detection_head.py:25\u001b[39m, in \u001b[36mSimpleDetectionHead.construct\u001b[39m\u001b[34m(self, feature_map)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconstruct\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature_map):\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Pass features through the shared layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Get the box and class predictions\u001b[39;00m\n\u001b[32m     28\u001b[39m     box_predictions = \u001b[38;5;28mself\u001b[39m.regressor(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\venv\\Lib\\site-packages\\mindspore\\nn\\cell.py:1355\u001b[39m, in \u001b[36mCell.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.requires_grad \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dynamic_shape_inputs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mixed_precision_type):\n\u001b[32m   1353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._forward_pre_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hook \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_hook \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1354\u001b[39m             \u001b[38;5;28mself\u001b[39m._shard_fn \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recompute_cell \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.has_bprop \u001b[38;5;129;01mand\u001b[39;00m _pynative_executor.requires_grad())):\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_construct(*args, **kwargs)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._complex_call(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\venv\\Lib\\site-packages\\mindspore\\nn\\layer\\conv.py:366\u001b[39m, in \u001b[36mConv2d.construct\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconstruct\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_bias:\n\u001b[32m    368\u001b[39m         output = \u001b[38;5;28mself\u001b[39m.bias_add(output, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\venv\\Lib\\site-packages\\mindspore\\ops\\primitive.py:413\u001b[39m, in \u001b[36mPrimitive.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_elim:\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\venv\\Lib\\site-packages\\mindspore\\ops\\primitive.py:1022\u001b[39m, in \u001b[36m_run_op\u001b[39m\u001b[34m(obj, op_name, args)\u001b[39m\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_op\u001b[39m(obj, op_name, args):\n\u001b[32m   1021\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Single op execution function supported by ge in PyNative mode.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     res = \u001b[43m_pynative_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_op_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;66;03m# Add for jit context.\u001b[39;00m\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m jit_context():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Codes\\TrafficVisionProject\\venv\\Lib\\site-packages\\mindspore\\common\\api.py:1638\u001b[39m, in \u001b[36m_PyNativeExecutor.run_op_async\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1628\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_op_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m   1629\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1630\u001b[39m \u001b[33;03m    Run single op async.\u001b[39;00m\n\u001b[32m   1631\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1636\u001b[39m \u001b[33;03m        StubNode, result of run op.\u001b[39;00m\n\u001b[32m   1637\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_op_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: For primitive[Conv2D], the x shape size must be equal to 4, but got 2.\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore\\core\\utils\\check_convert_utils.cc:626 mindspore::CheckAndConvertUtils::CheckInteger\n"
     ]
    }
   ],
   "source": [
    "# In main.py\n",
    "import mindspore as ms\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "from mindspore.dataset.transforms import TypeCast\n",
    "import mindspore.dataset.vision as vision\n",
    "\n",
    "# Import your custom dataset loader and the FINAL detector model\n",
    "from src.coco_dataset import CocoDataset\n",
    "from src.detector import TrafficVisionDetector  # <-- IMPORT THE NEW, COMPLETE MODEL\n",
    "\n",
    "# Set the context to CPU\n",
    "ms.set_context(device_target=\"CPU\",pynative_synchronize=True) # Added pynative_synchronize for easier debugging\n",
    "\n",
    "## 1. Prepare your COCO Dataset\n",
    "print(\"Initializing COCO dataset...\")\n",
    "annotations_path = './coco2017/annotations/instances_train2017.json'\n",
    "images_path = './coco2017/train2017'\n",
    "coco_ds = CocoDataset(annotations_file=annotations_path, images_dir=images_path)\n",
    "\n",
    "dataset = GeneratorDataset(\n",
    "    source=coco_ds,\n",
    "    column_names=[\"image\", \"boxes\", \"labels\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Define the image transformation pipeline\n",
    "transforms = [\n",
    "    vision.Resize((224, 224)),\n",
    "    TypeCast(ms.float32),\n",
    "    vision.HWC2CHW()\n",
    "]\n",
    "\n",
    "# Apply the transformations to the 'image' column.\n",
    "dataset = dataset.map(operations=transforms, input_columns=\"image\")\n",
    "\n",
    "# Now that images are a fixed size, you can batch them.\n",
    "dataset = dataset.batch(batch_size=1) # Using batch size of 1 for this test\n",
    "\n",
    "## 2. Create your FULL object detection model\n",
    "#    Your project has 8 traffic classes.\n",
    "print(\"\\nCreating the full TrafficVisionDetector model...\")\n",
    "model = TrafficVisionDetector(num_classes=8)\n",
    "model.set_train(False) # Set to evaluation mode for this test\n",
    "\n",
    "## 3. Test the flow of data through the entire model\n",
    "print(\"\\n--- Testing Full Model ---\")\n",
    "data_iterator = dataset.create_dict_iterator()\n",
    "data_batch = next(data_iterator)\n",
    "image_tensor = data_batch['image']\n",
    "\n",
    "print(f\"Loaded a preprocessed image with shape: {image_tensor.shape}\")\n",
    "\n",
    "# 4. Pass the image through the full model to get final predictions\n",
    "box_predictions, class_predictions = model(image_tensor)\n",
    "\n",
    "print(f\"\\nModel produced Box Predictions with shape: {box_predictions.shape}\")\n",
    "print(f\"Model produced Class Predictions with shape: {class_predictions.shape}\")\n",
    "print(\"\\nThese are the final outputs you will use in a loss function to train the model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
